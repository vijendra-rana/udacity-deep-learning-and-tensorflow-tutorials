{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\train-images-idx3-ubyte.gz\n",
      "Extracting data\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/\n",
    "#https://www.youtube.com/watch?v=vq2nnJ4g6N0&t=911s\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "mnist = read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape #here we have gray images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.validation.images.shape #so no validation set here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist.train.images) #so numpy array it should be simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,[None,28,28,1]) #we donot know here how many training images are we gonna give\n",
    "W = tf.Variable(tf.zeros([784,10])) #variables are that we want to change\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "init = tf.global_variables_initializer() #operation to initialize the variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model \n",
    "\n",
    "Y = tf.nn.softmax(tf.matmul(tf.reshape(X,[-1,784]),W) + b)\n",
    "\n",
    "Y_ = tf.placeholder(tf.float32,[None,10])  #correct labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss function\n",
    "cross_entropy = - tf.reduce_sum(Y_ * tf.log(Y)) # + 0.5 * tf.reduce_sum(tf.multiply(W,W)) #Adding regularization here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'Placeholder' type=Placeholder>,\n",
       " <tf.Operation 'zeros' type=Const>,\n",
       " <tf.Operation 'Variable' type=VariableV2>,\n",
       " <tf.Operation 'Variable/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable/read' type=Identity>,\n",
       " <tf.Operation 'zeros_1' type=Const>,\n",
       " <tf.Operation 'Variable_1' type=VariableV2>,\n",
       " <tf.Operation 'Variable_1/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_1/read' type=Identity>,\n",
       " <tf.Operation 'init' type=NoOp>,\n",
       " <tf.Operation 'Reshape/shape' type=Const>,\n",
       " <tf.Operation 'Reshape' type=Reshape>,\n",
       " <tf.Operation 'MatMul' type=MatMul>,\n",
       " <tf.Operation 'add' type=Add>,\n",
       " <tf.Operation 'Softmax' type=Softmax>,\n",
       " <tf.Operation 'Placeholder_1' type=Placeholder>,\n",
       " <tf.Operation 'Log' type=Log>,\n",
       " <tf.Operation 'mul' type=Mul>,\n",
       " <tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'Sum' type=Sum>,\n",
       " <tf.Operation 'Neg' type=Neg>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# % of correct answers found in the batch \n",
    "is_correct = tf.equal(tf.argmax(Y,1),tf.argmax(Y_,1)) #it will compute bool that is why cast is needed in the statement below\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#no execution so far\n",
    "#graph.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differed execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'GradientDescent' type=NoOp>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.gradient_descent.GradientDescentOptimizer at 0x16344960e10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to the execution------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config = config)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.500000\n"
     ]
    }
   ],
   "source": [
    "#just testing\n",
    "print(\"%f\"%0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration number 0 Training accuracy 0.440000 cross entropy loss 183.742615\n",
      "Iteration number 0 Testing accuracy 0.241000 cross entropy loss 20600.937500\n",
      "\n",
      "Iteration number 1000 Training accuracy 0.900000 cross entropy loss 34.714710\n",
      "Iteration number 1000 Testing accuracy 0.917600 cross entropy loss 2956.397949\n",
      "\n",
      "Iteration number 2000 Training accuracy 0.970000 cross entropy loss 15.753893\n",
      "Iteration number 2000 Testing accuracy 0.919400 cross entropy loss 2864.634766\n",
      "\n",
      "Iteration number 3000 Training accuracy 0.960000 cross entropy loss 21.403524\n",
      "Iteration number 3000 Testing accuracy 0.922600 cross entropy loss 2760.903564\n",
      "\n",
      "Iteration number 4000 Training accuracy 0.940000 cross entropy loss 25.103807\n",
      "Iteration number 4000 Testing accuracy 0.920700 cross entropy loss 2737.466797\n",
      "\n",
      "Iteration number 5000 Training accuracy 0.920000 cross entropy loss 22.589405\n",
      "Iteration number 5000 Testing accuracy 0.923800 cross entropy loss 2719.710205\n",
      "\n",
      "Iteration number 6000 Training accuracy 0.920000 cross entropy loss 26.929924\n",
      "Iteration number 6000 Testing accuracy 0.922100 cross entropy loss 2767.424072\n",
      "\n",
      "Iteration number 7000 Training accuracy 0.940000 cross entropy loss 31.343254\n",
      "Iteration number 7000 Testing accuracy 0.924100 cross entropy loss 2681.527832\n",
      "\n",
      "Iteration number 8000 Training accuracy 0.920000 cross entropy loss 22.947941\n",
      "Iteration number 8000 Testing accuracy 0.925000 cross entropy loss 2714.407471\n",
      "\n",
      "Iteration number 9000 Training accuracy 0.900000 cross entropy loss 30.356842\n",
      "Iteration number 9000 Testing accuracy 0.923900 cross entropy loss 2713.102295\n",
      "\n",
      "Iteration number 10000 Training accuracy 0.950000 cross entropy loss 16.703897\n",
      "Iteration number 10000 Testing accuracy 0.921800 cross entropy loss 2725.616699\n"
     ]
    }
   ],
   "source": [
    "for i in range(10001):\n",
    "    batch_X,batch_Y = mnist.train.next_batch(100)\n",
    "    train_data = {X : batch_X, Y_ : batch_Y}\n",
    "    \n",
    "    #training the system\n",
    "    sess.run(train_step , feed_dict = train_data)\n",
    "    \n",
    "    #success\n",
    "    a,c = sess.run([accuracy , cross_entropy],feed_dict= train_data)\n",
    "    if i % 1000 == 0:\n",
    "        print()\n",
    "        print(\"Iteration number %d Training accuracy %f cross entropy loss %f\"%(i,a,c))\n",
    "        #success on test data\n",
    "        \n",
    "        test_data = {X : mnist.test.images,Y_ : mnist.test.labels}\n",
    "        a,c = sess.run([accuracy,cross_entropy],feed_dict = test_data)\n",
    "        \n",
    "        print(\"Iteration number %d Testing accuracy %f cross entropy loss %f\"%(i,a,c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.truncated_normal([28*28,200],stddev= 0.1))\n",
    "B1 = tf.Variable(tf.zeros([200]))\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([200,10],stddev= 0.1))\n",
    "B2 = tf.Variable(tf.truncated_normal([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y1 =  tf.nn.sigmoid(tf.matmul(tf.reshape(X,[-1,784]),W1) + B1)\n",
    "Y_out = tf.nn.softmax(tf.matmul(Y1,W2) + B2)\n",
    "#copying from above\n",
    "\n",
    "Y_ = tf.placeholder(tf.float32,[None,10])  #correct labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#and now same thing as above\n",
    "cross_entropy = - tf.reduce_sum(Y_ * tf.log(Y_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(Y_out,1),tf.argmax(Y_,1)) #it will compute bool that is why cast is needed in the statement below\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this time lets define decaying learning rate\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.003\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                           5000, 0.96, staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy,global_step = global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration number 0 Training accuracy 0.160000 cross entropy loss 238.269150\n",
      "Iteration number 0 Testing accuracy 0.112000 cross entropy loss 26001.328125\n",
      "\n",
      "Iteration number 4000 Training accuracy 0.990000 cross entropy loss 6.191293\n",
      "Iteration number 4000 Testing accuracy 0.949500 cross entropy loss 1673.652710\n",
      "\n",
      "Iteration number 8000 Training accuracy 0.980000 cross entropy loss 5.255058\n",
      "Iteration number 8000 Testing accuracy 0.963400 cross entropy loss 1193.652222\n",
      "\n",
      "Iteration number 12000 Training accuracy 1.000000 cross entropy loss 5.236318\n",
      "Iteration number 12000 Testing accuracy 0.971100 cross entropy loss 924.843689\n",
      "\n",
      "Iteration number 16000 Training accuracy 1.000000 cross entropy loss 2.057953\n",
      "Iteration number 16000 Testing accuracy 0.974400 cross entropy loss 824.327759\n",
      "\n",
      "Iteration number 20000 Training accuracy 1.000000 cross entropy loss 2.411638\n",
      "Iteration number 20000 Testing accuracy 0.976200 cross entropy loss 728.297241\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(20001):\n",
    "    batch_X,batch_Y = mnist.train.next_batch(100)\n",
    "    train_data = {X : batch_X, Y_ : batch_Y}\n",
    "    \n",
    "    #training the system\n",
    "    sess.run(train_step , feed_dict = train_data)\n",
    "    \n",
    "    #success\n",
    "    a,c = sess.run([accuracy , cross_entropy],feed_dict= train_data)\n",
    "    if i % 4000 == 0:\n",
    "        print()\n",
    "        print(\"Iteration number %d Training accuracy %f cross entropy loss %f\"%(i,a,c))\n",
    "        #success on test data\n",
    "        \n",
    "        test_data = {X : mnist.test.images,Y_ : mnist.test.labels}\n",
    "        a,c = sess.run([accuracy,cross_entropy],feed_dict = test_data)\n",
    "        \n",
    "        print(\"Iteration number %d Testing accuracy %f cross entropy loss %f\"%(i,a,c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same thing but this time will use relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration number 0 Training accuracy 0.310000 cross entropy loss 252.920029\n",
      "Iteration number 0 Testing accuracy 0.215100 cross entropy loss 30305.580078\n",
      "\n",
      "Iteration number 4000 Training accuracy 1.000000 cross entropy loss 1.456518\n",
      "Iteration number 4000 Testing accuracy 0.976000 cross entropy loss 775.170654\n",
      "\n",
      "Iteration number 8000 Training accuracy 1.000000 cross entropy loss 0.380347\n",
      "Iteration number 8000 Testing accuracy 0.979700 cross entropy loss 680.427673\n",
      "\n",
      "Iteration number 12000 Training accuracy 1.000000 cross entropy loss 0.483505\n",
      "Iteration number 12000 Testing accuracy 0.981500 cross entropy loss 650.204041\n",
      "\n",
      "Iteration number 16000 Training accuracy 1.000000 cross entropy loss 0.282901\n",
      "Iteration number 16000 Testing accuracy 0.981900 cross entropy loss 669.653931\n",
      "\n",
      "Iteration number 20000 Training accuracy 1.000000 cross entropy loss 0.186634\n",
      "Iteration number 20000 Testing accuracy 0.982000 cross entropy loss 670.175720\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable(tf.truncated_normal([28*28,300],stddev= 0.1))\n",
    "B1 = tf.Variable(tf.ones([300])/10)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([300,10],stddev= 0.1))\n",
    "B2 = tf.Variable(tf.ones([10])/10)\n",
    "\n",
    "Y1 =  tf.nn.relu(tf.matmul(tf.reshape(X,[-1,784]),W1) + B1) #single line change\n",
    "Y_out = tf.nn.softmax(tf.matmul(Y1,W2) + B2)\n",
    "#copying from above\n",
    "\n",
    "Y_ = tf.placeholder(tf.float32,[None,10])  #correct labels\n",
    "\n",
    "#and now same thing as above\n",
    "cross_entropy = - tf.reduce_sum(Y_ * tf.log(Y_out))\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(Y_out,1),tf.argmax(Y_,1)) #it will compute bool that is why cast is needed in the statement below\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "#this time lets define decaying learning rate\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.003\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                           5000, 0.96, staircase=True)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy,global_step = global_step)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(20001):\n",
    "    batch_X,batch_Y = mnist.train.next_batch(100)\n",
    "    train_data = {X : batch_X, Y_ : batch_Y}\n",
    "    \n",
    "    #training the system\n",
    "    sess.run(train_step , feed_dict = train_data)\n",
    "    \n",
    "    #success\n",
    "    a,c = sess.run([accuracy , cross_entropy],feed_dict= train_data)\n",
    "    if i % 4000 == 0:\n",
    "        print()\n",
    "        print(\"Iteration number %d Training accuracy %f cross entropy loss %f\"%(i,a,c))\n",
    "        #success on test data\n",
    "        \n",
    "        test_data = {X : mnist.test.images,Y_ : mnist.test.labels}\n",
    "        a,c = sess.run([accuracy,cross_entropy],feed_dict = test_data)\n",
    "        \n",
    "        print(\"Iteration number %d Testing accuracy %f cross entropy loss %f\"%(i,a,c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Now that is kind of impressive 98.2 % accuracy \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This time we will use dropout and Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration number 0 Training accuracy 0.380000 cross entropy loss 175.315506\n",
      "Iteration number 0 Testing accuracy 0.248200 cross entropy loss 22249.929688\n",
      "\n",
      "Iteration number 4000 Training accuracy 0.110000 cross entropy loss nan\n",
      "Iteration number 4000 Testing accuracy 0.098000 cross entropy loss nan\n",
      "\n",
      "Iteration number 8000 Training accuracy 0.130000 cross entropy loss nan\n",
      "Iteration number 8000 Testing accuracy 0.098000 cross entropy loss nan\n",
      "\n",
      "Iteration number 12000 Training accuracy 0.080000 cross entropy loss nan\n",
      "Iteration number 12000 Testing accuracy 0.098000 cross entropy loss nan\n",
      "\n",
      "Iteration number 16000 Training accuracy 0.120000 cross entropy loss nan\n",
      "Iteration number 16000 Testing accuracy 0.098000 cross entropy loss nan\n",
      "\n",
      "Iteration number 20000 Training accuracy 0.110000 cross entropy loss nan\n",
      "Iteration number 20000 Testing accuracy 0.098000 cross entropy loss nan\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable(tf.truncated_normal([28*28,300],stddev= 0.1))\n",
    "B1 = tf.Variable(tf.ones([300])/10)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([300,10],stddev= 0.1))\n",
    "B2 = tf.Variable(tf.ones([10])/10)\n",
    "\n",
    "Y1 =  tf.nn.relu(tf.matmul(tf.reshape(X,[-1,784]),W1) + B1) #single line change\n",
    "Y1 = tf.nn.dropout(Y1,keep_prob= 0.7)\n",
    "Y_out = tf.nn.softmax(tf.matmul(Y1,W2) + B2)\n",
    "#copying from above\n",
    "\n",
    "Y_ = tf.placeholder(tf.float32,[None,10])  #correct labels\n",
    "\n",
    "#and now same thing as above\n",
    "cross_entropy = - tf.reduce_sum(Y_ * tf.log(Y_out))\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(Y_out,1),tf.argmax(Y_,1)) #it will compute bool that is why cast is needed in the statement below\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "#this time lets define decaying learning rate\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.003\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                           5000, 0.96, staircase=True)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy,global_step = global_step)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(20001):\n",
    "    batch_X,batch_Y = mnist.train.next_batch(100)\n",
    "    train_data = {X : batch_X, Y_ : batch_Y}\n",
    "    \n",
    "    #training the system\n",
    "    sess.run(train_step , feed_dict = train_data)\n",
    "    \n",
    "    #success\n",
    "    a,c = sess.run([accuracy , cross_entropy],feed_dict= train_data)\n",
    "    if i % 4000 == 0:\n",
    "        print()\n",
    "        print(\"Iteration number %d Training accuracy %f cross entropy loss %f\"%(i,a,c))\n",
    "        #success on test data\n",
    "        \n",
    "        test_data = {X : mnist.test.images,Y_ : mnist.test.labels}\n",
    "        a,c = sess.run([accuracy,cross_entropy],feed_dict = test_data)\n",
    "        \n",
    "        print(\"Iteration number %d Testing accuracy %f cross entropy loss %f\"%(i,a,c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code above gives loss as nan  solution use softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration number 0 Training accuracy 0.320000 cross entropy loss 194.676025\n",
      "Iteration number 0 Testing accuracy 0.202100 cross entropy loss 230.038956\n",
      "\n",
      "Iteration number 4000 Training accuracy 1.000000 cross entropy loss 0.777944\n",
      "Iteration number 4000 Testing accuracy 0.970600 cross entropy loss 10.565776\n",
      "\n",
      "Iteration number 8000 Training accuracy 1.000000 cross entropy loss 0.280782\n",
      "Iteration number 8000 Testing accuracy 0.971300 cross entropy loss 12.314648\n",
      "\n",
      "Iteration number 12000 Training accuracy 0.990000 cross entropy loss 1.266635\n",
      "Iteration number 12000 Testing accuracy 0.976300 cross entropy loss 13.126901\n",
      "\n",
      "Iteration number 16000 Training accuracy 1.000000 cross entropy loss 0.159158\n",
      "Iteration number 16000 Testing accuracy 0.976400 cross entropy loss 13.321301\n",
      "\n",
      "Iteration number 20000 Training accuracy 1.000000 cross entropy loss 0.120493\n",
      "Iteration number 20000 Testing accuracy 0.976600 cross entropy loss 15.962158\n"
     ]
    }
   ],
   "source": [
    "#copying from above\n",
    "W1 = tf.Variable(tf.truncated_normal([28*28,300],stddev= 0.1))\n",
    "B1 = tf.Variable(tf.ones([300])/10)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([300,10],stddev= 0.1))\n",
    "B2 = tf.Variable(tf.ones([10])/10)\n",
    "\n",
    "Y1 =  tf.nn.relu(tf.matmul(tf.reshape(X,[-1,784]),W1) + B1) #single line change\n",
    "Y1 = tf.nn.dropout(Y1,keep_prob= 0.7)\n",
    "#Y_out = tf.nn.softmax(tf.matmul(Y1,W2) + B2)\n",
    "\n",
    "#we will only calculate logits softmax will be done together with cross entropy in a numericaly stable way\n",
    "Y_logits = tf.matmul(Y1,W2) + B2\n",
    "\n",
    "Y_ = tf.placeholder(tf.float32,[None,10])  #correct labels\n",
    "\n",
    "#cross_entropy = - tf.reduce_sum(Y_ * tf.log(Y_out))\n",
    "#a new function https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#7\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = Y_logits,labels = Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy) * 100 #as suggested in the tutorial \n",
    "\n",
    "is_correct = tf.equal(tf.argmax(Y_logits,1),tf.argmax(Y_,1)) #it will compute bool that is why cast is needed in the statement below\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "#this time lets define decaying learning rate\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.003\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                           5000, 0.96, staircase=True)\n",
    "\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy,global_step = global_step)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(20001):\n",
    "    batch_X,batch_Y = mnist.train.next_batch(100)\n",
    "    train_data = {X : batch_X, Y_ : batch_Y}\n",
    "    \n",
    "    #training the system\n",
    "    sess.run(train_step , feed_dict = train_data)\n",
    "    \n",
    "    #success\n",
    "    a,c = sess.run([accuracy , cross_entropy],feed_dict= train_data)\n",
    "    \n",
    "    if i % 4000 == 0:\n",
    "        print()\n",
    "        #print(c)\n",
    "        print(\"Iteration number %d Training accuracy %f cross entropy loss %f\"%(i,a,c))\n",
    "        #print(\"Training iteration number\",i,\"accuracy\",a,\"loss\",c)\n",
    "        #success on test data\n",
    "        \n",
    "        test_data = {X : mnist.test.images,Y_ : mnist.test.labels}\n",
    "        a,c = sess.run([accuracy,cross_entropy],feed_dict = test_data)\n",
    "        \n",
    "        #print(\"Training iteration number\",i,\"accuracy\",a,\"loss\",c)\n",
    "        print(\"Iteration number %d Testing accuracy %f cross entropy loss %f\"%(i,a,c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5018"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.get_operations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now to Convolutions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#copying directly from the video\n",
    "K = 4\n",
    "L = 8\n",
    "M = 12\n",
    "\n",
    "\n",
    "#k number of filters\n",
    "W1 = tf.Variable(tf.truncated_normal([5,5,1,K],stddev= 0.1))\n",
    "B1 = tf.Variable(tf.ones([K])/10)\n",
    "\n",
    "\n",
    "#k channels in previous output and L number of filters\n",
    "W2 = tf.Variable(tf.truncated_normal([5,5,K,L],stddev= 0.1))\n",
    "B2 = tf.Variable(tf.ones([L])/10)\n",
    "\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([5,5,L,M],stddev= 0.1))\n",
    "B3 = tf.Variable(tf.ones([M])/10)\n",
    "\n",
    "#fully connected layer after this\n",
    "N = 200\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([7*7*M,N],stddev=0.1))\n",
    "B4 = tf.Variable(tf.ones([N])/10)\n",
    "\n",
    "W5 = tf.Variable(tf.truncated_normal([N,10],stddev= 0.1))\n",
    "B5 = tf.Variable(tf.ones([10])/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.get_operations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#here X is (100,28,28,1) 100 is #of images\n",
    "Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding= 'SAME') +B1) #output is (100,28,28,4)\n",
    "\n",
    "Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides = [1,2,2,1], padding= 'SAME') +B2) #output is (100,14,14,8)\n",
    "\n",
    "Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides = [1,2,2,1], padding= 'SAME') +B3) #output is (100,7,7,12)\n",
    "\n",
    "YY = tf.reshape(Y3, shape = [-1, 7 * 7 * M])  #essentially it will be (100,7*7*12)\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(YY,W4) + B4)                 #(100,200)\n",
    "#Y = tf.nn.softmax(tf.matmul(Y4,W5) + B5)               #(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration number 0 Training accuracy 0.150000 cross entropy loss 225.750488\n",
      "Iteration number 0 Testing accuracy 0.116300 cross entropy loss 237.257477\n",
      "\n",
      "Iteration number 500 Training accuracy 0.960000 cross entropy loss 13.901351\n",
      "Iteration number 500 Testing accuracy 0.974000 cross entropy loss 7.816738\n",
      "\n",
      "Iteration number 1000 Training accuracy 1.000000 cross entropy loss 1.759503\n",
      "Iteration number 1000 Testing accuracy 0.981300 cross entropy loss 5.586134\n",
      "\n",
      "Iteration number 1500 Training accuracy 0.980000 cross entropy loss 3.804555\n",
      "Iteration number 1500 Testing accuracy 0.982900 cross entropy loss 5.216575\n",
      "\n",
      "Iteration number 2000 Training accuracy 0.990000 cross entropy loss 2.985325\n",
      "Iteration number 2000 Testing accuracy 0.985900 cross entropy loss 4.586003\n",
      "\n",
      "Iteration number 2500 Training accuracy 1.000000 cross entropy loss 0.114356\n",
      "Iteration number 2500 Testing accuracy 0.986800 cross entropy loss 4.306031\n",
      "\n",
      "Iteration number 3000 Training accuracy 1.000000 cross entropy loss 1.594681\n",
      "Iteration number 3000 Testing accuracy 0.989200 cross entropy loss 3.534742\n",
      "\n",
      "Iteration number 3500 Training accuracy 1.000000 cross entropy loss 0.565449\n",
      "Iteration number 3500 Testing accuracy 0.987500 cross entropy loss 4.313298\n",
      "\n",
      "Iteration number 4000 Training accuracy 1.000000 cross entropy loss 0.395254\n",
      "Iteration number 4000 Testing accuracy 0.987200 cross entropy loss 4.890959\n"
     ]
    }
   ],
   "source": [
    "#we will do it with softmax with logits for numerical stability reasons\n",
    "Y_logits = tf.matmul(Y4,W5) + B5\n",
    "Y_ = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = Y_logits,labels = Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy) * 100 \n",
    "\n",
    "is_correct = tf.equal(tf.argmax(Y_logits,1),tf.argmax(Y_,1)) #it will compute bool that is why cast is needed in the statement below\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "#this time lets define decaying learning rate\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.003\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                           5000, 0.96, staircase=True)\n",
    "\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy,global_step = global_step)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(4001):\n",
    "    batch_X,batch_Y = mnist.train.next_batch(100)\n",
    "    train_data = {X : batch_X, Y_ : batch_Y}\n",
    "    \n",
    "    #training the system\n",
    "    sess.run(train_step , feed_dict = train_data)\n",
    "    \n",
    "    #success\n",
    "    a,c = sess.run([accuracy , cross_entropy],feed_dict= train_data)\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print()\n",
    "        #print(c)\n",
    "        print(\"Iteration number %d Training accuracy %f cross entropy loss %f\"%(i,a,c))\n",
    "        #print(\"Training iteration number\",i,\"accuracy\",a,\"loss\",c)\n",
    "        #success on test data\n",
    "        \n",
    "        test_data = {X : mnist.test.images,Y_ : mnist.test.labels}\n",
    "        a,c = sess.run([accuracy,cross_entropy],feed_dict = test_data)\n",
    "        \n",
    "        #print(\"Training iteration number\",i,\"accuracy\",a,\"loss\",c)\n",
    "        print(\"Iteration number %d Testing accuracy %f cross entropy loss %f\"%(i,a,c))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 98.72 that is kind of cool. Lets now try a different architecture and this time with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#copying directly from the video. Giving more degrees of freedom to the system \n",
    "#i.e. increasing the number of filters or number of Ws\n",
    "K = 6\n",
    "L = 12\n",
    "M = 24\n",
    "\n",
    "\n",
    "#k number of filters\n",
    "W1 = tf.Variable(tf.truncated_normal([6,6,1,K],stddev= 0.1))\n",
    "B1 = tf.Variable(tf.ones([K])/10)\n",
    "\n",
    "\n",
    "#k channels in previous output and L number of filters\n",
    "W2 = tf.Variable(tf.truncated_normal([5,5,K,L],stddev= 0.1))\n",
    "B2 = tf.Variable(tf.ones([L])/10)\n",
    "\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([4,4,L,M],stddev= 0.1))\n",
    "B3 = tf.Variable(tf.ones([M])/10)\n",
    "\n",
    "#fully connected layer after this\n",
    "N = 200\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([7*7*M,N],stddev=0.1))\n",
    "B4 = tf.Variable(tf.ones([N])/10)\n",
    "\n",
    "W5 = tf.Variable(tf.truncated_normal([N,10],stddev= 0.1))\n",
    "B5 = tf.Variable(tf.ones([10])/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "920"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.get_operations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here X is (100,28,28,1) 100 is #of images\n",
    "Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding= 'SAME') +B1) #output is (100,28,28,6)\n",
    "\n",
    "Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides = [1,2,2,1], padding= 'SAME') +B2) #output is (100,14,14,12)\n",
    "\n",
    "Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides = [1,2,2,1], padding= 'SAME') +B3) #output is (100,7,7,24)\n",
    "\n",
    "YY = tf.reshape(Y3, shape = [-1, 7 * 7 * M])  #essentially it will be (100,7*7*24)\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(YY,W4) + B4)                 #(100,200)\n",
    "#Y = tf.nn.softmax(tf.matmul(Y4,W5) + B5)               #(100,10)\n",
    "\n",
    "Y4 = tf.nn.dropout(Y4,keep_prob= 0.75)    #dropout introduced\n",
    "\n",
    "#we will do it with softmax with logits for numerical stability reasons\n",
    "Y_logits = tf.matmul(Y4,W5) + B5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration number 0 Training accuracy 0.130000 cross entropy loss 221.796997\n",
      "Iteration number 0 Testing accuracy 0.128100 cross entropy loss 231.528336\n",
      "\n",
      "Iteration number 5000 Training accuracy 1.000000 cross entropy loss 0.035542\n",
      "Iteration number 5000 Testing accuracy 0.989700 cross entropy loss 4.091547\n",
      "\n",
      "Iteration number 10000 Training accuracy 1.000000 cross entropy loss 0.003339\n",
      "Iteration number 10000 Testing accuracy 0.991500 cross entropy loss 4.168932\n",
      "\n",
      "Iteration number 15000 Training accuracy 1.000000 cross entropy loss 0.035732\n",
      "Iteration number 15000 Testing accuracy 0.991400 cross entropy loss 4.405719\n",
      "\n",
      "Iteration number 20000 Training accuracy 1.000000 cross entropy loss 0.000704\n",
      "Iteration number 20000 Testing accuracy 0.991500 cross entropy loss 4.513232\n"
     ]
    }
   ],
   "source": [
    "Y_ = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = Y_logits,labels = Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy) * 100 \n",
    "\n",
    "is_correct = tf.equal(tf.argmax(Y_logits,1),tf.argmax(Y_,1)) #it will compute bool that is why cast is needed in the statement below\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "#this time lets define decaying learning rate\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.003\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                           100, 0.96, staircase=True)\n",
    "\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy,global_step = global_step)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "#lets run for 10000\n",
    "for i in range(20001):\n",
    "    batch_X,batch_Y = mnist.train.next_batch(100)\n",
    "    train_data = {X : batch_X, Y_ : batch_Y}\n",
    "    \n",
    "    #training the system\n",
    "    sess.run(train_step , feed_dict = train_data)\n",
    "    \n",
    "    #success\n",
    "    a,c = sess.run([accuracy , cross_entropy],feed_dict= train_data)\n",
    "    \n",
    "    if i % 5000 == 0:\n",
    "        print()\n",
    "        #print(c)\n",
    "        print(\"Iteration number %d Training accuracy %f cross entropy loss %f\"%(i,a,c))\n",
    "        #print(\"Training iteration number\",i,\"accuracy\",a,\"loss\",c)\n",
    "        #success on test data\n",
    "        \n",
    "        test_data = {X : mnist.test.images,Y_ : mnist.test.labels}\n",
    "        a,c = sess.run([accuracy,cross_entropy],feed_dict = test_data)\n",
    "        \n",
    "        #print(\"Training iteration number\",i,\"accuracy\",a,\"loss\",c)\n",
    "        print(\"Iteration number %d Testing accuracy %f cross entropy loss %f\"%(i,a,c))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool 99.15 % finally key is decreasing learning rate and more number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre> \n",
    "This was further improved to 99.5% by the speaker with batch normalization\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
